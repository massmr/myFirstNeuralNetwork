Voici un copier-coller pr√™t √† l'emploi pour ton README, avec les formules en pseudo-LaTeX adapt√©es √† GitHub Markdown :  

```markdown
# Multilayer Neural Network  

## Description  
Ce projet impl√©mente un r√©seau de neurones multicouche en **PyTorch** pour une t√¢che de **classification binaire**.  
Le mod√®le apprend √† classifier des points g√©n√©r√©s al√©atoirement selon une r√®gle simple : la somme des coordonn√©es doit √™tre sup√©rieure √† 1 pour √™tre class√©e comme 1, sinon 0.  

## Installation  
### Pr√©requis  
- Python 3.x  
- PyTorch  
- NumPy  

### Installation des d√©pendances  
Assurez-vous que PyTorch est install√© sur votre machine. Si ce n'est pas le cas, installez-le avec :  
```bash
pip install torch numpy
```

## Utilisation  
### Ex√©cution du script  
Lancez simplement le script Python :  
```bash
python neural_network.py
```
Cela entra√Ænera le mod√®le pendant 50 √©poques et affichera la perte toutes les 10 √©poques, ainsi que la pr√©cision finale sur un ensemble de test.  

## Explication du Code  
### 1. G√©n√©ration des donn√©es  
Le jeu de donn√©es est g√©n√©r√© de mani√®re al√©atoire avec des points `(X1, X2)` entre 0 et 1.  
L'√©tiquette `y` est d√©finie comme :  
- `y = 1` si `X1 + X2 > 1`  
- `y = 0` sinon  

### 2. D√©finition du Mod√®le  
Le r√©seau est compos√© de **trois couches** :  
- Une couche cach√©e de **6 neurones** avec activation **ReLU**  
- Une seconde couche cach√©e de **4 neurones** avec **ReLU**  
- Une couche de sortie de **1 neurone** avec **sigmo√Øde** pour obtenir une probabilit√© entre 0 et 1  

### 3. Fonction co√ªt et optimisation  
- La **fonction co√ªt** utilis√©e est l'**entropie crois√©e binaire** (`BCELoss`).  
- L'**optimiseur** est **Adam** avec un taux d'apprentissage de `0.01`.  

### 4. Entra√Ænement du mod√®le  
Le r√©seau est entra√Æn√© pendant `50` √©poques en utilisant la **descente de gradient** avec r√©tropropagation.  

### 5. √âvaluation  
Apr√®s l'entra√Ænement, le mod√®le est test√© sur un ensemble de test et la pr√©cision est calcul√©e.  

## Explication Math√©matique  
### 1. Fonction d'activation  
- **ReLU** :  
  ```
  ReLU(x) = max(0, x)
  ```
  Utilis√©e pour √©viter le probl√®me de la **vanishing gradient**.  
- **Sigmo√Øde** :  
  ```
  œÉ(x) = 1 / (1 + e^(-x))
  ```
  Utilis√©e en sortie pour convertir la sortie en probabilit√©.  

### 2. Fonction Co√ªt (Binary Cross Entropy)  
```
L = - (1/N) * Œ£ [ y_i * log(yÃÇ_i) + (1 - y_i) * log(1 - yÃÇ_i) ]
```
avec :  
- `y_i` : vrai label (`0` ou `1`)  
- `≈∑_i` : pr√©diction du mod√®le  

## R√©sultats Attendus  
Apr√®s l'entra√Ænement, la pr√©cision sur les donn√©es de test devrait √™tre d'environ **80-90%**.  

## Auteur  
Projet r√©alis√© par **[Votre Nom]**.  
```

Tu peux directement coller √ßa dans ton README.md, et les formules s'afficheront correctement sur GitHub. üöÄ
